{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrY9clfB9F03"
   },
   "source": [
    "Prompts:\n",
    "\n",
    "Unzip the /content/Turkish_Movie_Sentiment.zip to my home folder\n",
    "\n",
    "Certainly, I'll add a cell to unzip the file to your home folder.\n",
    "Read each of the tr_polarity file content to a corresponding list, where each line of the file respresent a document, so you should read each line as a list item. The content is saved using windows turkish code page.\n",
    "\n",
    "I can help with that. I'll add code to read the content of tr_polarity.neg and tr_polarity.pos files into two lists, decoding them with the Windows Turkish code page.\n",
    "Print the first 10 negative documents\n",
    "\n",
    "Okay, here's the code to print the first 10 negative documents.\n",
    "try using \"iso-...\" something code page for Turkish that I dont remember exactly\n",
    "\n",
    "Okay, I can try using the \"iso-8859-9\" encoding, which is another common encoding for Turkish. I'll update the code cell to use this encoding.\n",
    "tokenize each document in place (i.e. use string.split()) whithout using any library.\n",
    "\n",
    "I understand. I will add a new cell to tokenize each document in both the negative and positive lists in place using the split() method.\n",
    "build a token2idx (i.e. token to index mapping) for the unique works over all documents.\n",
    "\n",
    "Okay, I will build a token2idx mapping for all unique words across the negative and positive documents.\n",
    "construct document vectors for each document in individual lists (pos and neg).\n",
    "\n",
    "I can help with that. I will construct document vectors for each document in the neg_list and pos_list using the token2idx mapping. I'll use a simple bag-of-words approach for this.\n",
    "please sort out the compute cells in the order of execution\n",
    "\n",
    "I cannot reorder the cells directly. However, the correct order of execution for the cells you have is:\n",
    "\n",
    "    Unzip the file: The cell with the !unzip command (f38cdd9e).\n",
    "    Read the files: The cell that reads the .neg and .pos files into lists (c85cb947).\n",
    "    Tokenize the documents: The cell that splits each document into a list of words (440e7a32).\n",
    "    Build the token2idx mapping: The cell that creates the dictionary mapping words to indices (eda63b29).\n",
    "    Construct document vectors: The cell that creates numerical vectors for each document (the last cell I generated).\n",
    "\n",
    "Please run them in this order.\n",
    "create document vectors again and put the cell to the end\n",
    "\n",
    "I can regenerate the code to create the document vectors and add it as a new cell at the end of your notebook.\n",
    "fill out all document vectors with the frequency of words in corresponding documents.\n",
    "\n",
    "I will modify the selected empty cell to construct document vectors based on word frequency using the existing token2idx mapping.\n",
    "check whether the document vectors are not all zero, i.e. there are some nonzero components in documents\n",
    "\n",
    "I will modify the selected empty cell to check if the document frequency vectors are all zeros or have non-zero components.\n",
    "Just sum each document in document vector lists\n",
    "\n",
    "Okay, I will modify the selected cell to print the sum of each document vector in both lists. This will show whether they have non-zero components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1759233822039,
     "user": {
      "displayName": "Bekir Taner Dinçer",
      "userId": "02382921844782906207"
     },
     "user_tz": -180
    },
    "id": "l2OV-mZFNSyy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /content/Turkish_Movie_Sentiment.zip, /content/Turkish_Movie_Sentiment.zip.zip or /content/Turkish_Movie_Sentiment.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# /content/Turkish_Movie_Sentiment.zip\n",
    "# unzip the above zip file to home folder\n",
    "\n",
    "!unzip /content/Turkish_Movie_Sentiment.zip -d /content/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1759239116684,
     "user": {
      "displayName": "Bekir Taner Dinçer",
      "userId": "02382921844782906207"
     },
     "user_tz": -180
    },
    "id": "c85cb947",
    "outputId": "98405b31-497a-4a3a-994c-0e4b8ef2d8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative documents: 5331\n",
      "Number of positive documents: 5331\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "neg_list = []\n",
    "pos_list = []\n",
    "\n",
    "neg_file_path = './Data/Turkish_Movie_Sentiment/tr_polarity.neg'\n",
    "pos_file_path = './Data/Turkish_Movie_Sentiment/tr_polarity.pos'\n",
    "\n",
    "with codecs.open(neg_file_path, 'r', encoding='iso-8859-9') as neg_file:\n",
    "    for line in neg_file:\n",
    "        neg_list.append(line.strip())\n",
    "\n",
    "with codecs.open(pos_file_path, 'r', encoding='iso-8859-9') as pos_file:\n",
    "    for line in pos_file:\n",
    "        pos_list.append(line.strip())\n",
    "\n",
    "print(f\"Number of negative documents: {len(neg_list)}\")\n",
    "print(f\"Number of positive documents: {len(pos_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1759234813800,
     "user": {
      "displayName": "Bekir Taner Dinçer",
      "userId": "02382921844782906207"
     },
     "user_tz": -180
    },
    "id": "2d2fb9fd",
    "outputId": "db053f30-2750-4514-ff3c-73539662d747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"giseye oynayan bir film.mel gibson'in oyunculugu yine çok kötü.film bastan sona duygu sömürüsü ama anlayan nerde!.\", 'bircok yonden sahip olduklari zayifliklari populerligi iyi kullanmasiyla gidermis zayif bir yapim...', '1995 ten bu yana bu tür filmler artti , o zamanlar basarili sayilabilecekse de suan emin degilim. konusu baya siradan ..', 'mel gibson tam bir ingiliz düsmani her filminde belirtiyor bunu. film yine cok iyi. biz kurtulus mücadelemizi böyle bir filmle simgeleyemedik. üzülüyorum bu yüzden..', 'milliyetçi bir film tavsiye etmiyorum....', 'saçma upuzun diyolaglar,kopuk sahneler,aniden biten bir film ve iste tarantino. .  ( 1 de çok ya,hadi neyse).', 'bu filme çok güsel, 10 numara diyen arkadaslar olmus ama sanirim ben onlarla ayni filmi seyretmedim :s oyuncu kadrosu ne kadar iyi olursa olsn, film çok kötü. film de ne anlatilio ne amaçlanio hiç bisi belli deil, bence vaktinizi hiç bosa harcamayin daha güsel filmler seyredin', 'filmi seyretmeden önce bu siteye baktim ve 9.1 aldigini gördüm.sonra gönül rahatligiyla izlemeye basladim.filmde 2 saat geçti ve hiçbir sey olmadi.heralde tarantino son yarim saatte çosturtmustur dedim.sonra bir baktim film bitmis.tek kelimeyle anlamsiz bir film.yazik 2,5 saatime.....', 'senaryosu bes para etmesede tarantinonun akilda kalici sekilde çekmesi insanlara filmin güzel oldugunu düsündürmekte.marjinal ama güzel degil.', 'uzun sikici diyaloglariyla klasik bir tarantino filmi evet kurgu güzel ama film hep uzun sikici diyaloglarla dolu. izlerken çooook sikildim açikcasi.']\n"
     ]
    }
   ],
   "source": [
    "print(neg_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f38cdd9e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1759239125154,
     "user": {
      "displayName": "Bekir Taner Dinçer",
      "userId": "02382921844782906207"
     },
     "user_tz": -180
    },
    "id": "440e7a32",
    "outputId": "747ce58d-3ee1-4197-f97a-808178474bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokenized negative documents:\n",
      "[['giseye', 'oynayan', 'bir', 'film.mel', \"gibson'in\", 'oyunculugu', 'yine', 'çok', 'kötü.film', 'bastan', 'sona', 'duygu', 'sömürüsü', 'ama', 'anlayan', 'nerde!.'], ['bircok', 'yonden', 'sahip', 'olduklari', 'zayifliklari', 'populerligi', 'iyi', 'kullanmasiyla', 'gidermis', 'zayif', 'bir', 'yapim...'], ['1995', 'ten', 'bu', 'yana', 'bu', 'tür', 'filmler', 'artti', ',', 'o', 'zamanlar', 'basarili', 'sayilabilecekse', 'de', 'suan', 'emin', 'degilim.', 'konusu', 'baya', 'siradan', '..'], ['mel', 'gibson', 'tam', 'bir', 'ingiliz', 'düsmani', 'her', 'filminde', 'belirtiyor', 'bunu.', 'film', 'yine', 'cok', 'iyi.', 'biz', 'kurtulus', 'mücadelemizi', 'böyle', 'bir', 'filmle', 'simgeleyemedik.', 'üzülüyorum', 'bu', 'yüzden..'], ['milliyetçi', 'bir', 'film', 'tavsiye', 'etmiyorum....'], ['saçma', 'upuzun', 'diyolaglar,kopuk', 'sahneler,aniden', 'biten', 'bir', 'film', 've', 'iste', 'tarantino.', '.', '(', '1', 'de', 'çok', 'ya,hadi', 'neyse).'], ['bu', 'filme', 'çok', 'güsel,', '10', 'numara', 'diyen', 'arkadaslar', 'olmus', 'ama', 'sanirim', 'ben', 'onlarla', 'ayni', 'filmi', 'seyretmedim', ':s', 'oyuncu', 'kadrosu', 'ne', 'kadar', 'iyi', 'olursa', 'olsn,', 'film', 'çok', 'kötü.', 'film', 'de', 'ne', 'anlatilio', 'ne', 'amaçlanio', 'hiç', 'bisi', 'belli', 'deil,', 'bence', 'vaktinizi', 'hiç', 'bosa', 'harcamayin', 'daha', 'güsel', 'filmler', 'seyredin'], ['filmi', 'seyretmeden', 'önce', 'bu', 'siteye', 'baktim', 've', '9.1', 'aldigini', 'gördüm.sonra', 'gönül', 'rahatligiyla', 'izlemeye', 'basladim.filmde', '2', 'saat', 'geçti', 've', 'hiçbir', 'sey', 'olmadi.heralde', 'tarantino', 'son', 'yarim', 'saatte', 'çosturtmustur', 'dedim.sonra', 'bir', 'baktim', 'film', 'bitmis.tek', 'kelimeyle', 'anlamsiz', 'bir', 'film.yazik', '2,5', 'saatime.....'], ['senaryosu', 'bes', 'para', 'etmesede', 'tarantinonun', 'akilda', 'kalici', 'sekilde', 'çekmesi', 'insanlara', 'filmin', 'güzel', 'oldugunu', 'düsündürmekte.marjinal', 'ama', 'güzel', 'degil.'], ['uzun', 'sikici', 'diyaloglariyla', 'klasik', 'bir', 'tarantino', 'filmi', 'evet', 'kurgu', 'güzel', 'ama', 'film', 'hep', 'uzun', 'sikici', 'diyaloglarla', 'dolu.', 'izlerken', 'çooook', 'sikildim', 'açikcasi.']]\n",
      "\n",
      "First 10 tokenized positive documents:\n",
      "[['gerçekten', 'harika', 'bir', 'yapim', 'birçok', 'kez', 'izledim', 'gene', 'izlerim', 'özgürlük', 'askini', 've', 'ingilizlerin', 'ne', 'kadar', 'vahset', 'olduklarini', 'gözler', 'önüne', 'seren', 'bir', 'film', 've', 'tabi', 'ki', 'ask....'], ['her', 'izledigimde', 'hayranlik', 'duydugum', 'gerçek', 'klasik', 'diyebilecegimiz', 'filmlerden', '.', 'içinde', 'teknik', 'hatalar', 'barindirsa', 'bile', 'sinema', 'olgusunun', 'en', 'üst', 'noktalarindan..'], ['gerçekten', 'tarihi', 'savas', 'filmleri', 'arasinda', 'tartismasiz', 'en', 'iyisi', ',', '12', 'yil', 'boyunca', 'acaba', 'ikincisi', 'çekirimi', 'diye', 'bekledigim', 'bir', 'film', ',belki', 'william', 'wallace', 'babasinin', 'ölümünden', 'sonra', 'amcasi', 'yanina', 'almisti', 'onu', 'yetistirmisti', 'belki', 'bunu', 'anlatan', 'mükkemmel', 'bir', 'filim', 'olablilr=).'], ['aldigi', 'ödülleri', 'sonuna', 'dek', 'hak', 'eden', 'muhtesem', 'bir', 'basyapit', '.'], ['özgürlük', 'denilince', 'aklima', 'gelen', 'ilk', 'film.bir', 'basyapit..'], ['sinema', 'tarihinin', 'gelmis', 'geçmis', 'en', 'büyük', 'en', 'büyük', 'bes', 'basyapitindan', 'biri', 've', 'en', 'iyi', 'tarihi', 'filmi.defalarca', 'defalarca', 'segredilen', 've', 'herseferindede', 'etkileyiciliginden', 've', 'büyüleyiciliginden', 'hiçbirsey', 'kaybetmeyen', 'kusursuz', 'bir', 'yapit..'], ['defalarca', 'kez', 'izlenecek', 'bir', 'basyapit..', '.', 'tarih', 'kahramanlari', 'asanlar', 'tarafindan', 'yazilir...', 'en', 'iyi', 'savas', 'filmidir', 'bana', 'göre..'], ['sinema', 'tarihinde', 'özgürlük,ihanet,intikam', 've', 'cesaret', 'temalarini', 'en', 'iyi', 'anlatan', 'bir', 'basyapit..'], ['bugün', 'tvde', 'yeniden', 'izledim', 'dvd', 'arsivindede', 'var.', 'sadece', 'tarihi', 'anlatmiyor', 'sinema', 'tarihinide', 'etkileyen', 'bir', 'film.', 'mel', 'gibsonun', 'en', 'iyi', 'filmi', '(ayrica', 'sonunda', 'öldügü', 'tek', 'filmi)..', 'ama', 'film', 'iskoçya', 'özgürlügünü', 'anlatmasina', 'ragmen', 'iskoçyalilara', 'sinir', 'oluyorsunuz', 'bir', 'adam', 'bu', 'kadarmi', 'satilir..'], ['söle', 'bisi', 'diyeyim', 'bu', 'filmi', 'begenmeyen', 'bi', 'daha', 'film', 'seyretmesin', 'harika', 'bir', 'film.']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize negative documents in place\n",
    "for i in range(len(neg_list)):\n",
    "    neg_list[i] = neg_list[i].split()\n",
    "\n",
    "# Tokenize positive documents in place\n",
    "for i in range(len(pos_list)):\n",
    "    pos_list[i] = pos_list[i].split()\n",
    "\n",
    "print(\"First 10 tokenized negative documents:\")\n",
    "print(neg_list[:10])\n",
    "print(\"\\nFirst 10 tokenized positive documents:\")\n",
    "print(pos_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1759239129395,
     "user": {
      "displayName": "Bekir Taner Dinçer",
      "userId": "02382921844782906207"
     },
     "user_tz": -180
    },
    "id": "eda63b29",
    "outputId": "2007c674-73d4-4929-c43a-c8c16bfc438d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 43704\n",
      "First 10 items in token2idx:\n",
      "[('uyudugum', 0), ('verelim...', 1), ('5.bölümünü', 2), ('izledim,vasatti', 3), ('baslasada', 4), ('ariyorum', 5), ('gerçek.inanilmaz', 6), ('koptu', 7), ('bile,hiç', 8), ('hanksin', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique words from both lists\n",
    "unique_words = set()\n",
    "for doc in neg_list + pos_list:\n",
    "    for word in doc:\n",
    "        unique_words.add(word)\n",
    "\n",
    "# Create the token2idx mapping\n",
    "token2idx = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "print(f\"Number of unique words: {len(unique_words)}\")\n",
    "print(\"First 10 items in token2idx:\")\n",
    "print(list(token2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1759239582686,
     "user": {
      "displayName": "Bekir Taner Dinçer",
      "userId": "02382921844782906207"
     },
     "user_tz": -180
    },
    "id": "6673db3d",
    "outputId": "ee552749-5793-47b4-b33e-16842fd17864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First negative document vector:\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "First positive document vector:\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to create a document vector\n",
    "def create_document_vector(document, token2idx):\n",
    "    vector = np.zeros(len(token2idx))\n",
    "    for word in document:\n",
    "        if word in token2idx:\n",
    "            vector[token2idx[word]] += 1\n",
    "    return vector\n",
    "\n",
    "# Create document vectors for negative documents\n",
    "neg_vectors = [create_document_vector(doc, token2idx) for doc in neg_list]\n",
    "\n",
    "# Create document vectors for positive documents\n",
    "pos_vectors = [create_document_vector(doc, token2idx) for doc in pos_list]\n",
    "\n",
    "print(\"First negative document vector:\")\n",
    "print(neg_vectors[0])\n",
    "print(\"\\nFirst positive document vector:\")\n",
    "print(pos_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55863,
     "status": "ok",
     "timestamp": 1759239893986,
     "user": {
      "displayName": "Bekir Taner Dinçer",
      "userId": "02382921844782906207"
     },
     "user_tz": -180
    },
    "id": "-7EtG4hn0_AC",
    "outputId": "78445006-2e22-489d-e75f-98ccd8693643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106109.0\n",
      "101488.0\n"
     ]
    }
   ],
   "source": [
    "print(sum([sum(doc) for doc in neg_vectors]))\n",
    "print(sum([sum(doc) for doc in pos_vectors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDXsw9i31q0e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM87IgJLrn9hPhZE5SSxUai",
   "mount_file_id": "1e6aJOfIobSJ6DmuQ49m5sBvQErZRuAQt",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
